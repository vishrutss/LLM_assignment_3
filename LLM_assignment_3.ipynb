{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fI3S9G4ijszFay-S6x9iUXVMSc1qmtfS",
      "authorship_tag": "ABX9TyOn+Sm1zparX2oLcLq7tKW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishrutss/LLM_assignment_3/blob/main/LLM_assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 3: Multilingual Large Language Models**\n",
        "\n",
        "## **Submitted by: Vishrut Sharma**"
      ],
      "metadata": {
        "id": "FPYStJBpoCXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) **Dataset Annotation**\n",
        "## **Dataset Creation:**\n",
        "* The dataset consists of sentiment analysis data from Twitter in 8 different languages.\n",
        "\n",
        "* It was designed to support tasks like text classification, specifically sentiment classification.\n",
        "\n",
        "* The data is split into training, validation, and test sets across all included languages.\n",
        "\n",
        "## **Potential weaknesses in annotation**\n",
        "* Subjectivity: Sentiment analysis can be subjective, and different annotators might interpret the sentiment of tweets differently.\n",
        "\n",
        "* Context Dependence: Tweets often contain slang, irony, or cultural references, which can be challenging to interpret correctly without additional context.\n",
        "\n",
        "* Language Nuances: With multiple languages involved, thereâ€™s a risk of misinterpreting sentiments due to language-specific nuances and expressions.\n",
        "\n",
        "* For example: The following tweet \"Listen to #NBAwards Winner @user interview on @user\" was given a positive sentiment. I feel that this tweet has a neutral sentiment because the tweet is just asking users to listen to an interview. But because the word winner is included in the tweet the annotator might have felt giving it a positive sentiment is appropriate."
      ],
      "metadata": {
        "id": "UcM-4AjyRHZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) **Language diversity**\n",
        "## **Predicted Performance (Best to Worst)**\n",
        "* English: English is likely to perform best due to its vast online presence and abundance of training data available beyond this dataset.\n",
        "\n",
        "* Spanish: Spanish also has a significant online presence and shares some cultural and linguistic roots with Portuguese, potentially benefiting from some transfer learning within the model.\n",
        "\n",
        "* French, German, Portuguese, Italian: These European languages have a decent online presence and might perform similarly. The model might leverage some similarities within these languages (French, Italian, Portuguese) for better performance.\n",
        "\n",
        "* Hindi: Hindi might perform slightly lower due to a potentially smaller online presence compared to the other languages. However, the sentiment analysis model might still perform adequately, especially for basic sentiment classification."
      ],
      "metadata": {
        "id": "99PYKXLnRMuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) **Multilingual sentiment analysis**\n"
      ],
      "metadata": {
        "id": "H6oycvYydf9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a) **Advanced prompting strategies**"
      ],
      "metadata": {
        "id": "B-FXPSs2qprR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "label_nums = [0, 1, 2]\n",
        "\n",
        "k = 25\n",
        "\n",
        "# Load train and test data\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/test.jsonl\", \"r\") as test_file:\n",
        "    test_data = [json.loads(line) for line in test_file]\n",
        "\n",
        "exemplars = {label: [] for label in label_nums}\n",
        "ground_truth_labels = []\n",
        "\n",
        "# Populate exemplars with the first k values of each class\n",
        "for entry in test_data:\n",
        "    label = int(entry[\"label\"])\n",
        "    if label!=1 and len(exemplars[label]) < k:\n",
        "        exemplars[label].append(entry[\"text\"])\n",
        "        ground_truth_labels.append(label)\n",
        "\n",
        "ground_truth_labels.sort()\n",
        "\n",
        "index_counter = 1\n",
        "\n",
        "for label, values in exemplars.items():\n",
        "    for value in values:\n",
        "        print(f\"{index_counter}) {value}\")\n",
        "        index_counter += 1"
      ],
      "metadata": {
        "id": "0leaHPunTA0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPT**"
      ],
      "metadata": {
        "id": "vaA4-mprdKxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regular prompt**\n",
        "**Prompt used**: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can be either positive or negative. Mention only the sentiment of each tweet.\""
      ],
      "metadata": {
        "id": "yA39mUq0M0-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "for word in [\"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "             \"Negative\", \"Positive\", \"Positive\", \"Negative\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"]:\n",
        "    predicted_labels.append(labels[word])\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd3ZRUAdYVvV",
        "outputId": "489cd22e-76f1-4742-a4a0-dad24d244aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8782467532467533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Emotion prompting**\n",
        "**Prompt used**: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. My job depends on you analyzing the sentiment accurately. The sentiment can be either positive or negative. Mention only the sentiment of each tweet.\"\n",
        "\n",
        "At first it denied and said it \"cannot fulfill the request\". I asked it why and it said that it is \"due to Open AI's policies against sentiment analysis or classification\" which was weird. I then repeated the same prompt as above along with the tweets and it immediatly responded with the sentiments. The policies must have changed pretty quickly.\n",
        "\n",
        "There was a considerable increase in performance with emotion prompting which is fascinating."
      ],
      "metadata": {
        "id": "9Pk2RdEUNC1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "for word in [\"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"]:\n",
        "    predicted_labels.append(labels[word])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVoFMH8QLfFK",
        "outputId": "9e11168e-fee4-4c3e-aa26-7a6728a19ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.9599358974358975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero shot CoT**\n",
        "Prompt used \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. Let's think step by step. The sentiment can ONLY be either positive or negative. Mention only the sentiment of each tweet (Positive or Negative). Do not mention neutral.\"\n",
        "\n",
        "This is the best performance so far for ChatGPT with an F1 score of 0.979. Adding the sentence \"Let's think step by step\" seems to have increased performance considerably. I imagine when this sentence gets added to the prompt the model breaks up the tweet into chunks and analyzes it, as the prompt suggests, \"step by step\" and self generates a rationale which it can then use for other tweets in the test set.\n",
        "\n",
        "I also had to assert at the end not to mention neutral because on many tries it would generate neutral predictions. I did this because I wanted to see how it would perform when it was forced to not sit on the fence for some tweets. It looks like for the most part it performed very well."
      ],
      "metadata": {
        "id": "aITc7_39pfFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Negative\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBf3emZJpi0q",
        "outputId": "e7abd243-2ddc-4481-c98f-4ebe9a9c9442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.9799919967987194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gemini**"
      ],
      "metadata": {
        "id": "QQGrBT87dbtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regular prompt**\n",
        "Prompt used \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can be either positive or negative. Mention only the sentiment of each tweet.\""
      ],
      "metadata": {
        "id": "3NYUGiECPz5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "for word in [\"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Negative\", \"Negative\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Negative\", \"Positive\", \"Positive\", \"Positive\"]:\n",
        "    predicted_labels.append(labels[word])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hczsdg7TddzH",
        "outputId": "624e338d-db8b-400f-acd5-ea6d29eedfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8193496587715776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Emotion prompting**\n",
        "**Prompt used**: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. My job depends on you analyzing the sentiment accurately. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet.\"\n",
        "\n",
        "Very slight increase in performance and this increase might be random chance but it is still interesting to see. The increase is also not as much as ChatGPT."
      ],
      "metadata": {
        "id": "6fOa7CK5SGgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "for word in [\"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Positive\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "             \"Negative\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "             \"Positive\", \"Negative\", \"Positive\", \"Positive\", \"Positive\"]:\n",
        "    predicted_labels.append(labels[word])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lBLfmKZQuEU",
        "outputId": "d68d0a3e-0e6c-49b0-abc9-4586fcf1bab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8585858585858586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero Shot CoT**\n",
        "**Prompt used** \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. Let's think step by step. The sentiment can ONLY be either positive or negative. Mention only the sentiment of each tweet (Positive or Negative). Do not mention neutral.\"\n",
        "\n",
        "Similar to ChatGPT using zero shot CoT prompting has increased the performance in Gemini. Although it is still a small increase as compared to ChatGPT it is nevertheless an increase.\n",
        "\n",
        "This is also the best performance for gemini so far with an F1 score of 0.899"
      ],
      "metadata": {
        "id": "SK0XIwnosBk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Negative\", \"Positive\", \"Negative\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH1yxNP9sFV0",
        "outputId": "4aeb428a-c20b-486c-824e-b1047df29b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8999599839935974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Graph plot**"
      ],
      "metadata": {
        "id": "Ro4TJ3rEkN6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "prompt_styles = [\"Regular prompt\", \"Emotion prompt\", \"Zero shot CoT\"]\n",
        "chatgpt_f1_scores = [0.878, 0.959, 0.979]\n",
        "gemini_f1_scores = [0.819, 0.858, 0.899]\n",
        "\n",
        "f1_scores = chatgpt_f1_scores + gemini_f1_scores\n",
        "\n",
        "x = np.arange(len(prompt_styles))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.bar(x - width/2, chatgpt_f1_scores, width, label='ChatGPT', color='blue')\n",
        "ax.bar(x + width/2, gemini_f1_scores, width, label='Gemini', color='red')\n",
        "\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('F1 Scores by Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(prompt_styles)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "V_lTvTizhYeC",
        "outputId": "1f86bf68-95b1-4885-a202-0783a5b7b2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# b) **Multilingual prompting**"
      ],
      "metadata": {
        "id": "xjRN8VKsg1fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "label_nums = [0, 1, 2]\n",
        "\n",
        "k = 25\n",
        "\n",
        "# Load train and test data. Change according to need\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/es_test.jsonl\", \"r\") as test_file:\n",
        "    test_data = [json.loads(line) for line in test_file]\n",
        "\n",
        "exemplars = {label: [] for label in label_nums}\n",
        "ground_truth_labels = []\n",
        "\n",
        "# Populate exemplars with the first k values of each class\n",
        "for entry in test_data:\n",
        "    label = int(entry[\"label\"])\n",
        "    if label!=1 and len(exemplars[label]) < k:\n",
        "        exemplars[label].append(entry[\"text\"])\n",
        "        ground_truth_labels.append(label)\n",
        "\n",
        "ground_truth_labels.sort()\n",
        "\n",
        "index_counter = 1\n",
        "\n",
        "for label, values in exemplars.items():\n",
        "    for value in values:\n",
        "        print(f\"{index_counter}) {value}\")\n",
        "        index_counter += 1"
      ],
      "metadata": {
        "id": "e7px72VohYWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPT**"
      ],
      "metadata": {
        "id": "77KvY4IRhIoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English prompt with german test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet.\n",
        "Remember there are 50 tweets:\"\n",
        "\n",
        "An interesting thing about the prompt I used is that I had to remind ChatGPT that I provided 50 tweets as it consistently only produced sentiments for 48-49 tweets.\n",
        "\n",
        "Nevertheless the performance seems to be very good with an english prompt."
      ],
      "metadata": {
        "id": "L8646-w4hNWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "for value in [\"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "              \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "              \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "              \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "              \"Negative\", \"Negative\", \"Negative\", \"Positive\", \"Positive\",\n",
        "              \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "              \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "              \"Positive\", \"Negative\", \"Positive\", \"Positive\", \"Positive\",\n",
        "              \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "              \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Negative\"]:\n",
        "    predicted_labels.append(labels[value])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VacVWfnOhHs3",
        "outputId": "2de904d9-9828-4f8b-fbf1-1d80b1bca113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **German prompt with german test set**\n",
        "**Prompt used**: \"Fungieren Sie als Stimmungsanalysator. Analysieren Sie fÃ¼r jeden der folgenden 50 Tweets die Stimmung. Die Stimmung kann nur entweder positiv oder negativ sein. ErwÃ¤hnen Sie nur die Stimmung jedes Tweets.\n",
        "Denken Sie daran, es gibt 50 Tweets:\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "Giving a prompt in german for the german test set massively improved the F1 score. What is surprising to me is that this performance is better than the english prompt used for the english test set in the previous section which has an F1-score of 0.878. This might have been a random occurrence but it is still very interesting."
      ],
      "metadata": {
        "id": "S7RKQ0aNj8o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativ\": 0, \"Positiv\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "for value in [\"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\",\n",
        "              \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\",\n",
        "              \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\",\n",
        "              \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\",\n",
        "              \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\", \"Positiv\",\n",
        "              \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "              \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "              \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "              \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "              \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Negativ\"]:\n",
        "    predicted_labels.append(labels[value])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h_ew7jPj7zm",
        "outputId": "66f84dcf-575b-46d1-837e-f17a1f083e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English prompt with French test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. Mention only the sentiment of each tweet (Positive or Negative). Do NOT mention Neutral. Remember there are 50 tweets.\"\n",
        "\n",
        "I was very surprised at the low performance of ChatGPT on the French dataset. I imagined the performance would be at least on par with English because French is also a high resource language."
      ],
      "metadata": {
        "id": "ZCIRy9GdlPRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Positive\", \"Positive\", \"Negative\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maI9-9EVlWuo",
        "outputId": "2fc69923-a120-4801-e2c8-d10011db1021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7181964573268921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **French prompt with French test set**\n",
        "Prompt used: \"Agir en tant quâ€™analyseur de sentiments. Pour chacun des 50 tweets suivants, analysez le sentiment. Mentionnez uniquement le sentiment de chaque tweet (positif ou nÃ©gatif). Ne mentionnez PAS Neutre. N'oubliez pas qu'il y a 50 tweets.\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "A small increase in performance is seen when a french prompt is used on the dataset. But this is still one of the weakest performances by ChatGPT."
      ],
      "metadata": {
        "id": "h_ANGY43noKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"NÃ©gatif\": 0, \"Positif\": 2}\n",
        "\n",
        "values = [\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"Positif\", \"Positif\", \"NÃ©gatif\", \"Positif\",\n",
        "    \"Positif\", \"Positif\", \"Positif\", \"Positif\", \"Positif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\", \"Positif\", \"Positif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"Positif\", \"Positif\",\n",
        "    \"Positif\", \"Positif\", \"Positif\", \"Positif\", \"Positif\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QSTjCN-o3ym",
        "outputId": "6cd42686-7730-4d15-f3f3-b050daa15a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7596153846153846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English prompt with Hindi test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. Mention only the sentiment of each tweet (Positive or Negative). Do NOT mention Neutral. Remember there are 50 tweets.\"\n",
        "\n",
        "Even with specific instructions the model predicted neutral for a few tweets which caused the F1 score to be fairly average."
      ],
      "metadata": {
        "id": "R_s1UIq8vx74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Neutral\":1, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Neutral\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Neutral\", \"Neutral\", \"Negative\",\n",
        "    \"Neutral\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Neutral\", \"Negative\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Negative\", \"Neutral\",\n",
        "    \"Neutral\", \"Neutral\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Negative\", \"Negative\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Neutral\", \"Positive\", \"Positive\", \"Neutral\",\n",
        "    \"Neutral\", \"Neutral\", \"Positive\", \"Negative\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfjYr_Ekv4Ob",
        "outputId": "91e6d436-3248-4ee2-8f23-7e9b10aa1cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7421052631578948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hindi prompt with Hindi test set**\n",
        "Prompt used: \"ek bhaavana vishleshak ke roop mein kaary karen. nimnalikhit 50 tveets mein se pratyek ke lie bhaavana ka vishleshan karen. pratyek tveet kee bhaavana (sakaaraatmak ya nakaaraatmak) ka hee ullekh karen. tatasth ka ullekh na karen. yaad rakhen 50 tveet hain.\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "There is a good increase in performance when Hindi prompt is used. An interesting observation I had was the model started to explain the reason for each sentiment prediction even though I had not prompted it to."
      ],
      "metadata": {
        "id": "E378oedNv4dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Nakaaraatmak\": 0, \"Sakaaraatmak\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\",\n",
        "    \"Nakaaraatmak\", \"Nakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Nakaaraatmak\",\n",
        "    \"Sakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\",\n",
        "    \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\",\n",
        "    \"Nakaaraatmak\", \"Nakaaraatmak\", \"Nakaaraatmak\", \"Sakaaraatmak\", \"Nakaaraatmak\",\n",
        "    \"Sakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\",\n",
        "    \"Sakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\",\n",
        "    \"Sakaaraatmak\", \"Sakaaraatmak\", \"Nakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\",\n",
        "    \"Sakaaraatmak\", \"Nakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Nakaaraatmak\",\n",
        "    \"Nakaaraatmak\", \"Sakaaraatmak\", \"Sakaaraatmak\", \"Nakaaraatmak\", \"Sakaaraatmak\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CkFzNOKv7YU",
        "outputId": "588c463b-9409-4c57-d2a8-b015e0b68697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8199279711884754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English Prompt Italian test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet. Remember there are 50 tweets:\n",
        "\""
      ],
      "metadata": {
        "id": "wr6FHv8sup_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Positive\", \"Negative\", \"Negative\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Negative\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdhjDUqIvXlm",
        "outputId": "84a429b3-8298-450f-e56c-883ab3145c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8999599839935974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Italian prompt Italian test set**\n",
        "Prompt used: \"Agire come un analizzatore del sentiment. Per ognuno dei successivi 50 tweet analizziamo il sentiment. Il sentimento puÃ² essere solo positivo o negativo. Menziona solo il sentimento di ogni tweet. Ricorda che ci sono 50 tweet:\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "A slight decrease in performance when compared to the english prompt."
      ],
      "metadata": {
        "id": "wiwxN834vX5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativo\": 0, \"Positivo\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Negativo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Negativo\", \"Positivo\", \"Positivo\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rM30c-ivdd4",
        "outputId": "ee06037b-2d4e-4ecb-fd1f-f6a62ffc4825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8798076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English Prompt Portuguese test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet. Remember there are 50 tweets:\n",
        "\""
      ],
      "metadata": {
        "id": "VS9wqvt8y5ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positive\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Positive\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Positive\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Positive\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB9nNMAUy_tr",
        "outputId": "4fe17029-e409-44b9-f2c8-d6a85f1dd81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8782467532467533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Portuguese prompt Portuguese test set**\n",
        "Prompt used: \"Atuar como um analisador de sentimento. Para cada um dos 50 tweets a seguir, analise o sentimento. O sentimento sÃ³ pode ser positivo ou negativo. Mencione apenas o sentimento de cada tweet. Lembre-se de que existem 50 tweets:\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "Similar to Italian a slight decrease in performance when compared to the english prompt."
      ],
      "metadata": {
        "id": "BToLXJU0y_7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativo\": 0, \"Positivo\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Positivo\", \"Positivo\", \"Negativo\",\n",
        "    \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7S0fu4azAI4",
        "outputId": "1d92af7f-f0a5-4e38-edfd-a94068a8187d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8572011423908609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English Prompt Spanish test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet. Remember there are 50 tweets:\n",
        "\"\n",
        "\n",
        "Perfect sentiment prediction."
      ],
      "metadata": {
        "id": "oyf92yKd1XiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzGZ0hFx1XxI",
        "outputId": "9cd114dd-cd9a-4c42-c0f7-ed3d64de7379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Spanish Prompt Spanish test set**\n",
        "\n",
        "Prompt used: \"Actuar como analizador de sentimientos. Para cada uno de los siguientes 50 tweets, analice el sentimiento. El sentimiento sÃ³lo puede ser positivo o negativo. Mencione solo el sentimiento de cada tweet. Recuerda que son 50 tweets:\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "A considerable decrease in performance when spanish prompt is used."
      ],
      "metadata": {
        "id": "xmlBN1AA1X9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativo\": 0, \"Positivo\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positivo\", \"Negativo\", \"Negativo\", \"Positivo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Positivo\", \"Negativo\", \"Positivo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Negativo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Negativo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEVHPJ8J1YLr",
        "outputId": "175a85b6-d619-4546-adba-3db5e691acb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8389694041867954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gemini**"
      ],
      "metadata": {
        "id": "kfcD7Y1BoYy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English prompt with german test set**\n",
        "**Prompt used**: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet in the provided order.\"\n",
        "\n",
        "When I gave gemini this prompt it generated 2 drafts. The first draft contained neutral predictions as well even though I specifically mentioned in the prompt that the sentiment can only be either positive or negative.\n",
        "\n",
        "The second draft gave me the expected output. Although it performed much worse than ChatGPT. And this is an interesting thing about Gemini which is that it can generate multiple drafts which have varied performance and results."
      ],
      "metadata": {
        "id": "ToMdekEzA7Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "values = [\"Negative\", \"Positive\", \"Negative\", \"Negative\", \"Negative\",\n",
        "          \"Negative\", \"Negative\", \"Positive\", \"Negative\", \"Negative\",\n",
        "          \"Negative\", \"Negative\", \"Negative\", \"Positive\", \"Negative\",\n",
        "          \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "          \"Positive\", \"Positive\", \"Positive\", \"Negative\", \"Positive\",\n",
        "          \"Positive\", \"Negative\", \"Positive\", \"Negative\", \"Positive\",\n",
        "          \"Negative\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "          \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "          \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "          \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"]\n",
        "\n",
        "for value in values:\n",
        "    predicted_labels.append(labels[value])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUNqY5muBAUn",
        "outputId": "82944a6b-d3b5-464c-f435-c26d07f8a567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7777777777777779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **German prompt with german test set:**\n",
        "**Prompt used**: \"Fungieren Sie als Stimmungsanalysator. Analysieren Sie fÃ¼r jeden der folgenden 50 Tweets die Stimmung. Die Stimmung kann nur entweder positiv oder negativ sein. ErwÃ¤hnen Sie nur die Stimmung jedes Tweets.\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "Compared to ChatGPT the prompt being in the same language did not really help with the performance for Gemini. This could mean that regardless of the language used Gemini performs at about the same level. This could be a good thing in the sense that the user need not know the language of the dataset or text they want to analyze to get better performance.\n",
        "\n",
        "But I would still prefer ChatGPT because even with an English prompt for the german test set it outperforms Gemini comfortably."
      ],
      "metadata": {
        "id": "HH1As-yUDakW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativ\": 0, \"Positiv\": 2}\n",
        "\n",
        "predicted_labels = []\n",
        "values = [\"Positiv\", \"Negativ\", \"Negativ\", \"Negativ\", \"Negativ\",\n",
        "          \"Negativ\", \"Negativ\", \"Positiv\", \"Negativ\", \"Negativ\",\n",
        "          \"Negativ\", \"Positiv\", \"Negativ\", \"Negativ\", \"Negativ\",\n",
        "          \"Negativ\", \"Negativ\", \"Positiv\", \"Negativ\", \"Positiv\",\n",
        "          \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "          \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "          \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "          \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "          \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\",\n",
        "          \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\", \"Positiv\"]\n",
        "\n",
        "for value in values:\n",
        "    predicted_labels.append(labels[value])\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YM_UG44DyYw",
        "outputId": "2b629af9-407e-4723-eef6-a5b3f4b33455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7916666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English prompt with French test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. Mention only the sentiment of each tweet (Positive or Negative). Do NOT mention Neutral. Remember there are 50 tweets.\"\n",
        "\n",
        "An interesting observation is that even though I provided the prompt in english the model responded in French. ChatGPT had responded in english but Gemini seems to assume that I expect an answer in French because there is a lot of French text present.\n",
        "\n",
        "The performance is still very poor for French in Gemini as well. The model also generated some neutral prediction even with specific instructions not to which is interesting."
      ],
      "metadata": {
        "id": "TpD8aWh9s9bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"NÃ©gatif\": 0, \"Neutre\": 1, \"Positif\": 2}\n",
        "\n",
        "values = [\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\",\n",
        "    \"Positif\", \"Neutre\", \"Positif\", \"Positif\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\", \"Neutre\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"Positif\", \"Positif\",\n",
        "    \"Positif\", \"Positif\", \"Positif\", \"Neutre\", \"Positif\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7zEQdRstDjF",
        "outputId": "cbb868a7-6479-4b14-cd96-701c415e7b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.6986359761295823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **French prompt with French test set**\n",
        "Prompt used: \"Agir en tant quâ€™analyseur de sentiments. Pour chacun des 50 tweets suivants, analysez le sentiment. Mentionnez uniquement le sentiment de chaque tweet (positif ou nÃ©gatif). Ne mentionnez PAS Neutre. N'oubliez pas qu'il y a 50 tweets.\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "Almost the same level of performance is seen even when a French prompt is used. The same behaviour present in German seems to be present here as well which is that regardless of the language used for the prompt the performance will generally be the same."
      ],
      "metadata": {
        "id": "JdxcXtg7tDzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"NÃ©gatif\": 0, \"Neutre\": 1, \"Positif\": 2}\n",
        "\n",
        "values = [\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\",\n",
        "    \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\", \"NÃ©gatif\",\n",
        "    \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\", \"NÃ©gatif\", \"Positif\",\n",
        "    \"Positif\", \"Neutre\", \"Positif\", \"Positif\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Neutre\", \"NÃ©gatif\", \"Positif\", \"NÃ©gatif\",\n",
        "    \"Neutre\", \"NÃ©gatif\", \"Positif\", \"Neutre\", \"NÃ©gatif\",\n",
        "    \"Positif\", \"Positif\", \"NÃ©gatif\", \"Positif\", \"Positif\",\n",
        "    \"Positif\", \"Positif\", \"Positif\", \"Neutre\", \"Positif\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OWN-TnetLTZ",
        "outputId": "62c7182c-9392-47ac-fb1c-2eab3ffee109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.6911111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English prompt with Hindi test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. Mention only the sentiment of each tweet (Positive or Negative). Do NOT mention Neutral. Remember there are 50 tweets.\"\n",
        "\n",
        "After trying multiple times and opening new chats as well Gemini refuses to analyze the sentiment of the tweets stating: \"Mujhe is vishay mein madad karne ke liye program nahin kiya gaya hai.\" which translates to \"I am not programmed to help with this topic\"."
      ],
      "metadata": {
        "id": "O1qiOsTgymyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hindi prompt with Hindi test set**\n",
        "Prompt used: \"ek bhaavana vishleshak ke roop mein kaary karen. nimnalikhit 50 tveets mein se pratyek ke lie bhaavana ka vishleshan karen. pratyek tveet kee bhaavana (sakaaraatmak ya nakaaraatmak) ka hee ullekh karen. tatasth ka ullekh na karen. yaad rakhen 50 tveet hain.\"\n",
        "\n",
        "As expected Gemini again refuses to analyze the sentiment of the tweets stating \"Main ismein aapki madad nahin kar sakta, kyunki main sirf ek language model hoon aur mujhe is vishay ke baare mein zaroori jaankari nahin hai.\" which translates to \"I cannot help you with this, because I am just a Language model and I do not have the necessary information on this topic\"\n",
        "\n",
        "I wonder if this is because Hindi is a relatively low resource language when compared to the other languages in the dataset and Google just doesn't want to risk generating incorrect responses for tasks like sentiment analysis and just has Gemini refuse the task."
      ],
      "metadata": {
        "id": "fbGLiXTuyoJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English Prompt Italian test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet. Remember there are 50 tweets:\n",
        "\"\n",
        "\n",
        "Similar to Hindi Gemini denied the request stating \"Non posso aiutarti, poichÃ© sono solo un modello linguistico e non ho la capacitÃ  di elaborare e comprendere questa richiesta.\" which translates to \"I cannot help you, as I am only a language model and do not have the ability to process and understand this request.\""
      ],
      "metadata": {
        "id": "Ya485QO2xyuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Italian prompt Italian test set**\n",
        "Prompt used: \"Agire come un analizzatore del sentiment. Per ognuno dei successivi 50 tweet analizziamo il sentiment. Il sentimento puÃ² essere solo positivo o negativo. Menziona solo il sentimento di ogni tweet. Ricorda che ci sono 50 tweet:\"\n",
        "\n",
        "As expected the model denied the request stating \"Come modello linguistico, non posso aiutarti in questo caso.\" which translates to \"As a language model, I can't help you in this case.\""
      ],
      "metadata": {
        "id": "-67sLUhxx7MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English Prompt Portuguese test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet. Remember there are 50 tweets:\n",
        "\"\n",
        "\n",
        "This is the best performance by Gemini so far across all languages."
      ],
      "metadata": {
        "id": "YIIq8Pzbz196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positive\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Positive\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Positive\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEgZNQxr0cft",
        "outputId": "4b839ecb-d6ba-420a-ed8e-56dcce33c0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.9194847020933977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Portuguese prompt Portuguese test set**\n",
        "Prompt used: \"Atuar como um analisador de sentimento. Para cada um dos 50 tweets a seguir, analise o sentimento. O sentimento sÃ³ pode ser positivo ou negativo. Mencione apenas o sentimento de cada tweet. Lembre-se de que existem 50 tweets:\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "Similar to ChatGPT a slight decrease in performance when compared to the english prompt."
      ],
      "metadata": {
        "id": "ocmP4fqSz2HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativo\": 0, \"Positivo\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Positivo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mRJYS0U0wZk",
        "outputId": "b92472eb-8f07-491c-875d-9d5ab7ec126a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8782467532467533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **English Prompt Spanish test set**\n",
        "Prompt used: \"Act as a sentiment analyzer. For each of the following 50 tweets analyze the sentiment. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet. Remember there are 50 tweets:\n",
        "\""
      ],
      "metadata": {
        "id": "EvrrHAJc3naj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positive\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Positive\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Positive\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Negative\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzLEYZ9Q37Ko",
        "outputId": "5ed6fdb5-1a55-41c5-b67a-7cf86033f8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.8996386993175431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Spanish Prompt Spanish test set**\n",
        "\n",
        "Prompt used: \"Actuar como analizador de sentimientos. Para cada uno de los siguientes 50 tweets, analice el sentimiento. El sentimiento sÃ³lo puede ser positivo o negativo. Mencione solo el sentimiento de cada tweet (Positivo o Negativo). Recuerda que son 50 tweets:\"\n",
        "\n",
        "The prompt was translated using google translate.\n",
        "\n",
        "A slight increase in performance compared to the english prompt."
      ],
      "metadata": {
        "id": "42j4EZUs3npq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negativo\": 0, \"Positivo\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Positivo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Positivo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\",\n",
        "    \"Negativo\", \"Negativo\", \"Negativo\", \"Negativo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\",\n",
        "    \"Negativo\", \"Positivo\", \"Positivo\", \"Positivo\", \"Positivo\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfhnucnp37-X",
        "outputId": "4f28d96d-5611-4e78-b7ce-297a0084a001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.9198717948717947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph plot and analysis**\n",
        "**German**: German prompt performed better than English prompt in ChatGPT. Both around the same in Gemini.\n",
        "\n",
        "**French**: French prompt performed better than English prompt in ChatGPT. Both around the same in Gemini.\n",
        "\n",
        "**Hindi**: Hindi prompt performed better than English prompt in ChatGPT. Denied request in Gemini.\n",
        "\n",
        "**Italian**: English prompt performed slightly better than Italian prompt in ChatGPT. Denied request in Gemini.\n",
        "\n",
        "**Portuguese**: English prompt performed slightly better for both models.\n",
        "\n",
        "**Spanish**: English prompt performed better than Spanish prompt in ChatGPT. Spanish prompt performed slightly better for Gemini."
      ],
      "metadata": {
        "id": "WeXOu4W4kFEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "prompt_styles = [\"En, De\", \"De, De\",\n",
        "                 \"En, Fr\", \"Fr, Fr\",\n",
        "                 \"En, Hi\", \"Hi, Hi\",\n",
        "                 \"En, It\", \"It, It\",\n",
        "                 \"En, Por\", \"Por, Por\",\n",
        "                 \"En, Es\", \"Es, Es\"]\n",
        "chatgpt_f1_scores = [0.88, 0.96, 0.718, 0.759, 0.742, 0.819, 0.899, 0.879, 0.878,\n",
        "                     0.857, 1.0, 0.838]\n",
        "gemini_f1_scores = [0.777, 0.791, 0.698, 0.691, 0, 0, 0, 0, 0.919, 0.878, 0.899,\n",
        "                    0.919]\n",
        "\n",
        "x = np.arange(len(prompt_styles))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "ax.bar(x - width/2, chatgpt_f1_scores, width, label='ChatGPT', color='blue')\n",
        "ax.bar(x + width/2, gemini_f1_scores, width, label='Gemini', color='red')\n",
        "\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_xlabel('Prompt Language, Tweets Language')\n",
        "ax.set_title('F1 Scores by Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(prompt_styles)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "EsOdMDyXja0X",
        "outputId": "7e71ca0a-d75f-4f67-bb4d-6dfda15cd894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **c) Few-shot in-context learning**"
      ],
      "metadata": {
        "id": "7dEeSJa-N-_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "label_nums = [0, 1, 2]\n",
        "\n",
        "k_test = 25\n",
        "k_train = 4\n",
        "\n",
        "# Load train and test data\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/test.jsonl\", \"r\") as test_file, open(\"/content/drive/MyDrive/Colab Notebooks/train.jsonl\", \"r\") as train_file:\n",
        "    test_data = [json.loads(line) for line in test_file]\n",
        "    train_data = [json.loads(line) for line in train_file]\n",
        "\n",
        "exemplars_test = {label: [] for label in label_nums}\n",
        "exemplars_train = {label: [] for label in label_nums}\n",
        "ground_truth_labels = []\n",
        "\n",
        "# Populate exemplars with the first k values of each class\n",
        "for entry in test_data:\n",
        "    label = int(entry[\"label\"])\n",
        "    if label!=1 and len(exemplars_test[label]) < k_test:\n",
        "        exemplars_test[label].append(entry[\"text\"])\n",
        "        ground_truth_labels.append(label)\n",
        "\n",
        "for entry in train_data:\n",
        "    label = int(entry[\"label\"])\n",
        "    if label!=1 and len(exemplars_train[label]) < k_train:\n",
        "        exemplars_train[label].append(entry[\"text\"] + \" Sentiment is \" + labels[label])\n",
        "\n",
        "ground_truth_labels.sort()\n",
        "\n",
        "print(\"Test Set\\n\")\n",
        "index_counter = 1\n",
        "\n",
        "for label, values in exemplars_test.items():\n",
        "    for value in values:\n",
        "        print(f\"{index_counter}) {value}\")\n",
        "        index_counter += 1\n",
        "\n",
        "print(\"\\nTrain Set\\n\")\n",
        "index_counter = 1\n",
        "\n",
        "for label, values in exemplars_train.items():\n",
        "    for value in values:\n",
        "        print(f\"{index_counter}) {value}\")\n",
        "        index_counter += 1"
      ],
      "metadata": {
        "id": "Qk58Rz70OM4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPT**\n",
        "**Prompt used**: \"Act as a sentiment analyzer. Consider the following examples: {train set examples}. Now analyze the sentiment of the 50 tweets provided below. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet in the provided order.\"\n",
        "\n",
        "There is a considerable decrease in performance when I provided the model with examples. This behaviour seems consistent with the BLOOM models where I observed a similar decrease in performance when I provided examples to the model.\n",
        "\n",
        "This decrease in performance could also be random chance or the prompt used might not have been effective and might have confused the model."
      ],
      "metadata": {
        "id": "sq2aLtWiQwOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Positive\",\n",
        "          \"Positive\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "          \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "          \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "          \"Negative\", \"Positive\", \"Negative\", \"Negative\", \"Positive\",\n",
        "          \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Negative\",\n",
        "          \"Negative\", \"Positive\", \"Positive\", \"Positive\", \"Negative\",\n",
        "          \"Positive\", \"Positive\", \"Negative\", \"Positive\", \"Positive\",\n",
        "          \"Negative\", \"Negative\", \"Negative\", \"Positive\", \"Positive\",\n",
        "          \"Positive\", \"Negative\", \"Positive\", \"Positive\", \"Positive\"]\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X9ZZrT6QabP",
        "outputId": "fb87e397-a9bc-4d41-db7e-6f6151f8b944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.7584541062801933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gemini**\n",
        "**Prompt used**: \"Act as a sentiment analyzer. Consider the following examples: {train set examples}. Now analyze the sentiment of the 50 tweets provided below. The sentiment can only be either positive or negative. Mention only the sentiment of each tweet in the provided order.\"\n",
        "\n",
        "The interesting thing about Gemini's response is it mentioned the tweets and gave an explaination as to why it thought the sentiment was positive or negative even though I never asked the model to explain.\n",
        "\n",
        "Gemini seems to shine with few shot prompting. It performs considerably better than ChatGPT and it also performed better than itself when no examples were given which was to be expected."
      ],
      "metadata": {
        "id": "diV_KCTESD8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = {\"Negative\": 0, \"Positive\": 2}\n",
        "\n",
        "values = [\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Negative\", \"Negative\",\n",
        "    \"Negative\", \"Negative\", \"Negative\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Negative\", \"Negative\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\",\n",
        "    \"Positive\", \"Positive\", \"Positive\", \"Positive\", \"Positive\"\n",
        "]\n",
        "\n",
        "predicted_labels = [labels[value] for value in values]\n",
        "\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 score:\",f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFLfMflkSGOu",
        "outputId": "0d3d5658-8100-45b8-f639-51ce4d907751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph plot**"
      ],
      "metadata": {
        "id": "qobYFg5yaw_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\"ChatGPT\", \"Gemini\"]\n",
        "f1_scores = [0.758, 0.92]\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "x = np.arange(len(models))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.bar(x, f1_scores, width, color=colors)\n",
        "\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('F1 Scores by Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "Uqo_Q0gHbgv_",
        "outputId": "07a0232a-6221-4c0f-ffdf-644a453687f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d) **Explainable AI**"
      ],
      "metadata": {
        "id": "8x2IHhT_c7ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "label_nums = [0, 1, 2]\n",
        "\n",
        "k = 10\n",
        "\n",
        "# Load train and test data\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/test.jsonl\", \"r\") as test_file:\n",
        "    test_data = [json.loads(line) for line in test_file]\n",
        "\n",
        "exemplars = {label: [] for label in label_nums}\n",
        "ground_truth_labels = []\n",
        "\n",
        "# Populate exemplars with the first k values of each class\n",
        "for entry in test_data:\n",
        "    label = int(entry[\"label\"])\n",
        "    if label!=1 and len(exemplars[label]) < k:\n",
        "        exemplars[label].append(entry[\"text\"])\n",
        "        ground_truth_labels.append(label)\n",
        "\n",
        "ground_truth_labels.sort()\n",
        "\n",
        "index_counter = 1\n",
        "\n",
        "for label, values in exemplars.items():\n",
        "    for value in values:\n",
        "        print(f\"{index_counter}) {value}\")\n",
        "        index_counter += 1"
      ],
      "metadata": {
        "id": "YpmHhLtwyrfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPT**\n",
        "\n",
        "## **Prompt used**:\n",
        "Act as a sentiment analyzer. Analyze the sentiment of all the provided tweets. The sentiment can either be positive or negative. Also explain the reasoning behind the classification\n",
        "by highlighting specific words or phrases that influence the sentiment prediction.\n",
        "\n",
        "## **ChatGPT response**:\n",
        "\n",
        "Based on the provided tweets, here are the sentiment analyses:\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: Words like \"pointless\" and \"infuriating\" convey a negative sentiment towards the conversation about vegetarianism. Additionally, the hashtag #caveman implies frustration or backwardness.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: Being blocked by someone (Flynn) and the use of \"isn't too pleased\" indicate a negative sentiment in this tweet.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet seems critical and implies frustration or dissatisfaction with the situation described.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet highlights a situation where Israel deems a comatose Gaza man a security threat, which suggests a negative view of Israel's actions in Gaza.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet indicates impending military action in Mosul, suggesting a tense or negative situation.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet expresses a negative sentiment towards Trump, using hashtags like #rejecttrump and #notmypresident, along with supporting movements like #blacklivesmatter.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet opposes fracking and criticizes Cuadrilla, implying a negative sentiment towards their actions.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet suggests concern about the future and implies a negative outlook on the world's dependence on fossil fuels.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet criticizes society for being publicly distasteful, indicating a negative sentiment towards societal behavior.\n",
        "\n",
        "Sentiment: Negative\n",
        "Reasoning: The tweet expresses disapproval or incredulity towards Ben Carson's appointment for Housing & Urban Development.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet compliments Vice President Pence, indicating a positive sentiment towards him.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet expresses admiration for Pope Francis, indicating a positive sentiment towards him.\n",
        "\n",
        "Sentiment: Neutral\n",
        "Reasoning: The tweet provides information about Samsung's plans, without conveying a clear positive or negative sentiment.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet praises the TV show Westworld, indicating a positive sentiment towards it.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet expresses love for Chick-fil-A, indicating a positive sentiment towards the restaurant.\n",
        "\n",
        "Sentiment: Neutral\n",
        "Reasoning: The tweet seems informational without conveying a clear positive or negative sentiment.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet expresses pride and support for the Trump transition team, indicating a positive sentiment towards them.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet expresses gratitude or appreciation for working with someone named TK, indicating a positive sentiment towards them.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet expresses approval or agreement with Microsoft's decision, indicating a positive sentiment towards it.\n",
        "\n",
        "Sentiment: Positive\n",
        "Reasoning: The tweet expresses enjoyment of the TV show Scream Queens, indicating a positive sentiment towards it.\n",
        "\n",
        "## **Analysis**\n",
        "I think ChatGPT did an amaxing job predicting the sentiment of each of the tweet. It did make a few neutral predictions even though I mentioned that the sentiment can be either positive or negative but the netral sentiment predictions felt accurate.\n",
        "\n",
        "The explainations it provided for each tweets also felt very accurate.\n",
        "\n",
        "So far for me ChatGPT has only performed poorly with few shot prompting but I feel that getting it to explain the reasoning behind each prediction made the model perform even better. The model predicted almost all the sentiments accurately with a few neutral sentiments."
      ],
      "metadata": {
        "id": "lmz2HGcx2MMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gemini**\n",
        "\n",
        "## **Prompt used**:\n",
        "Act as a sentiment analyzer. Analyze the sentiment of all the provided tweets. The sentiment can either be positive or negative. Also explain the reasoning behind the classification by highlighting specific words or phrases that influence the sentiment prediction.\n",
        "\n",
        "## **Gemini response**:\n",
        "\n",
        "Sentiment Analysis of Tweets:\n",
        "\n",
        "**1. Negative**\n",
        "\n",
        "* Reasoning: Words like \"pointless,\" \"infuriating,\" and \"caveman\" convey frustration and negativity towards the conversation.\n",
        "\n",
        "**2. Negative**\n",
        "\n",
        "* Reasoning: \"Blocked\" and \"not pleased\" indicate a negative interaction between the users.\n",
        "\n",
        "**3. Negative**\n",
        "\n",
        "* Reasoning: The tweet criticizes someone for \"crying\" and implies negativity towards \"middle America.\"\n",
        "\n",
        "**4. Negative**\n",
        "\n",
        "* Reasoning: Phrases like \"comatose,\" \"needs treatment,\" and \"security threat\" highlight a concerning situation.\n",
        "* Hashtag #Palestine suggests a potential conflict.\n",
        "\n",
        "**5. Neutral**\n",
        "\n",
        "* Reasoning: While the tweet mentions military action, it lacks emotional words and focuses on factual information.\n",
        "\n",
        "**6. Negative**\n",
        "\n",
        "* Reasoning: Hashtags like #rejecttrump and #nevertrump express disapproval towards Trump.\n",
        "\n",
        "**7. Negative**\n",
        "\n",
        "* Reasoning: Words like \"fracking,\" \"persecuting,\" \"BankruptCuadrilla,\" and #ECOCIDE portray a negative view of a certain practice.\n",
        "\n",
        "**8. Negative**\n",
        "\n",
        "* Reasoning: \"Crisis\" and \"run out\" suggest a pessimistic outlook on the future.\n",
        "\n",
        "**9. Negative**\n",
        "\n",
        "* Reasoning: \"Casually comfortable about being publicly distasteful\" expresses disapproval of societal behavior. #deplorables is a negative term.\n",
        "\n",
        "**10. Negative**\n",
        "\n",
        "* Reasoning: Emojis  \"\" and \"\" indicate disappointment with Ben Carson's appointment.\n",
        "\n",
        "**11. Positive**\n",
        "\n",
        "* Reasoning: Phrases like \"stand up guy\" and \"Gentleman\" express positive qualities.\n",
        "\n",
        "**12. Positive**\n",
        "\n",
        "* Reasoning: The speaker feels comforted by Pope Francis and desires a hug, suggesting positive emotions.\n",
        "\n",
        "**13. Neutral**\n",
        "\n",
        "* Reasoning: This tweet conveys factual information about a software update.\n",
        "\n",
        "**14. Positive**\n",
        "\n",
        "* Reasoning: Words like \"brilliant,\" \"good,\" and \"storytelling\" express a positive evaluation of Westworld.\n",
        "\n",
        "**15. Positive**\n",
        "\n",
        "* Reasoning: Emoji \"\" and the name \"Chick-fil-A\" suggest positive feelings about fast food.\n",
        "\n",
        "**16. Neutral**\n",
        "\n",
        "* Reasoning: This tweet announces a news segment but doesn't express positive or negative sentiment.\n",
        "\n",
        "**17. Positive**\n",
        "\n",
        "* Reasoning: Hashtags like #TeamTrump and #MakeAmericaGreatAgain indicate support for Trump's team.\n",
        "\n",
        "**18. Positive**\n",
        "\n",
        "* Reasoning: \"Lucky\" and \"get to work with\" imply positive feelings about working with someone.\n",
        "\n",
        "**19. Positive**\n",
        "\n",
        "* Reasoning: \"Big deal\" and \"smart move\" suggest a positive assessment of Microsoft's action.\n",
        "\n",
        "**20. Positive**\n",
        "\n",
        "* Reasoning: All caps \"SO SO GOOD\" expresses strong positive feelings about the show.\n",
        "\n",
        "\n",
        "## **Analysis**\n",
        "Similar to ChatGPT I think Gemini also did a good job predicting the sentiments of the tweets provided. I think when the model has to explain it's decision it does much better at predicting. But there are insitances like the following reasoning given by gemini \"Reasoning: While the tweet mentions military action, it lacks emotional words and focuses on factual information.\", where the annotator might argue that a military action should make this a negative sentiment tweet but gemini thinks this is just factual information. This is completely subjective and the sentiment will differ with different viewpoints and it is something we need to consider with tasks like sentiment analysis."
      ],
      "metadata": {
        "id": "_UhFoTIT3xYv"
      }
    }
  ]
}